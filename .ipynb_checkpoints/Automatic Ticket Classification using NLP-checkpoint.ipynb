{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for data loading, data viz and EDA\n",
    "import json \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for text preprocessing and analysis\n",
    "import re,nltk,spacy,string\n",
    "from nltk.corpus import stopwords\n",
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import NMF\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import swifter\n",
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for model evaluation metrics\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# row and column display limit\n",
    "pd.set_option('display.max_rows',None)\n",
    "pd.set_option('display.max_columns',None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = open('complaints.json')\n",
    "  \n",
    "data = json.load(j)\n",
    " \n",
    "df = pd.json_normalize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing _ From Column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for str_loop in df.columns:\n",
    "#     print(\"str_loop\",str_loop)\n",
    "    if str_loop.startswith(\"_\"):\n",
    "        renamed_col = str_loop.lstrip('_')\n",
    "        df.rename(columns={str_loop:renamed_col}, inplace=True)\n",
    "print(\"df cols renamed\",df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the number of missing values percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(df.isna().sum()*100/78313,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are many blank rows in 'source.complaint_what_happened'. Converting them into NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['source.complaint_what_happened'].replace(\"\", np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping NaN rows from \"source.complaint_what_happened\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['source.complaint_what_happened'], inplace=True)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign new column names\n",
    "df.rename(columns={'source.complaint_what_happened':'complaints_what_happened', 'source.product':'tag'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the text for topic modeling\n",
    "#### Once you have removed all the blank complaints, you need to:\n",
    "\n",
    "- Make the text lowercase\n",
    "- Remove text in square brackets\n",
    "- Remove punctuation\n",
    "- Remove words containing numbers\n",
    "- Once you have done these cleaning operations you need to perform the following:\n",
    "\n",
    "#### Lemmatize the texts\n",
    "Use POS tags to get relevant words from the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_texts(text):\n",
    "    #Make the text lowercase\n",
    "    text=text.lower()\n",
    "    \n",
    "    #Remove text in square brackets\n",
    "    text=re.sub(r'\\[.*?\\]','',text)\n",
    "    \n",
    "    #Remove punctuation\n",
    "    text=re.sub(r'[%s]%re.escape(string.punctuation)','',text)\n",
    "    \n",
    "    #Remove words containing numbers\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma_texts(text):     \n",
    "        \n",
    "    # Initialize empty list to store lemmas\n",
    "    lemma_list = []\n",
    "    \n",
    "    # Extract lemmas of given text and add to the list 'sent'\n",
    "    document = nlp(text)\n",
    "    for word in document:\n",
    "        lemma_list.append(word.lemma_)\n",
    "        \n",
    "    # return string converted form of the list of lemmas\n",
    "    return \" \".join(lemma_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['complaints'] = df['complaints_what_happened'].swifter.apply(clean_texts)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column for lemmatized complaints to the dataframe\n",
    "df[\"lemmatized_complaint\"] =  df.swifter.apply(lambda x: lemma_texts(x['complaints_what_happened']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean=df[['complaints_what_happened','lemmatized_complaint']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Function to extract the POS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_tags(text):\n",
    "    nn_words = []\n",
    "    doc = nlp(text)\n",
    "    for tok in doc:\n",
    "        if(tok.tag_ == 'NN'):\n",
    "            nn_words.append(tok.lemma_)\n",
    "    nn_words_str = \" \".join(nn_words)\n",
    "    return nn_words_str\n",
    "\n",
    "#this column should contain lemmatized text with all the words removed which have tags other than NN[tag == \"NN\"].\n",
    "df_clean[\"complaint_POS_removed\"] =  df_clean.swifter.apply(lambda x: get_pos_tags(x['lemmatized_complaint']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise the data according to the 'Complaint' character length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "doc_lens = [len(d) for d in df_clean.complaint_POS_removed]\n",
    "plt.hist(doc_lens, bins = 50)\n",
    "plt.ylabel('Number of Complaint')\n",
    "plt.xlabel('Complaint character length')\n",
    "sns.despine();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worcloud of most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(STOPWORDS)\n",
    "word_cloud = WordCloud(\n",
    "                          background_color='white',\n",
    "                          stopwords=stop_words,\n",
    "                          max_font_size=40,\n",
    "                          max_words=50, \n",
    "                          random_state=100\n",
    "                         ).generate(str(df_clean['complaint_POS_removed']))\n",
    "\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "plt.imshow(word_cloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top unigrams,bigrams and trigrams by frequency among all the complaints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_unigram(text, n=30):\n",
    "\n",
    "    vector = CountVectorizer(stop_words='english').fit(text)\n",
    "    bag_of_words = vector.transform(text)\n",
    "    sum_of_words = bag_of_words.sum(axis=0) \n",
    "    word_freq = [(word, sum_of_words[0, idx]) for word, idx in vector.vocabulary_.items()]\n",
    "    word_freq =sorted(word_freq, key = lambda x: x[1], reverse=True)\n",
    "    return word_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_common_words = get_top_unigram(df_clean['complaint_POS_removed'].values.astype('U'))\n",
    "df_unigram = pd.DataFrame(top_common_words, columns = ['unigram' , 'count'])\n",
    "df_unigram.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "sns.barplot(x='unigram', y='count', data=df_unigram, palette=\"Reds_d\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Top 30 unigrams in the Complaint text\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your code here to find the top 30 bigram frequency among the complaints in the cleaned datafram(df_clean). \n",
    "def get_top_bigram(text, n=30):\n",
    "\n",
    "    vector = CountVectorizer(ngram_range=(2, 2), stop_words='english').fit(text)\n",
    "    bag_of_words = vector.transform(text)\n",
    "    sum_of_words = bag_of_words.sum(axis=0) \n",
    "    word_freq = [(word, sum_of_words[0, idx]) for word, idx in vector.vocabulary_.items()]\n",
    "    word_freq =sorted(word_freq, key = lambda x: x[1], reverse=True)\n",
    "    return word_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_common_words = get_top_bigram(df_clean['complaint_POS_removed'].values.astype('U'))\n",
    "df_bigram = pd.DataFrame(top_common_words, columns = ['Bigram' , 'count'])\n",
    "df_bigram.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "sns.barplot(x='Bigram', y='count', data=df_bigram, palette=\"Blues_d\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Top 30 bigrams in the Complaint text\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_trigram(text, n=30):\n",
    "\n",
    "    vector = CountVectorizer(ngram_range=(3, 3), stop_words='english').fit(text)\n",
    "    bag_of_words = vector.transform(text)\n",
    "    sum_of_words = bag_of_words.sum(axis=0) \n",
    "    word_freq = [(word, sum_of_words[0, idx]) for word, idx in vector.vocabulary_.items()]\n",
    "    word_freq =sorted(word_freq, key = lambda x: x[1], reverse=True)\n",
    "    return word_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_common_words = get_top_trigram(df_clean['complaint_POS_removed'].values.astype('U'))\n",
    "df_trigram = pd.DataFrame(top_common_words, columns = ['trigram' , 'count'])\n",
    "df_trigram.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "sns.barplot(x='trigram', y='count', data=df_trigram, palette=\"Greens_d\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Top 30 trigrams in the Complaint text\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The personal details of customer has been masked in the dataset with xxxx. Let's remove the masked text as it won't help us in analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['complaint_POS_removed'] = df_clean['complaint_POS_removed'].str.replace('xxxx','')\n",
    "df_clean['complaint_POS_removed'] = df_clean['complaint_POS_removed'].str.replace('XXXX','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_clean.to_excel(\"df_clean.xlsx\")\n",
    "# df_clean=pd.read_excel(\"df_clean.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert the raw texts to a matrix of TF-IDF features\n",
    "\n",
    "- max_df is used for removing terms that appear too frequently, also known as \"corpus-specific stop words\" max_df = 0.95 means \"ignore terms that appear in more than 95% of the complaints\"\n",
    "\n",
    "- min_df is used for removing terms that appear too infrequently min_df = 2 means \"ignore terms that appear in less than 2 complaints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a document term matrix using fit_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = tfidf.fit_transform(df_clean['complaint_POS_removed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modelling using NMF\n",
    "- Non-Negative Matrix Factorization (NMF) is an unsupervised technique so there are no labeling of topics that the model will be trained on. The way it works is that, NMF decomposes (or factorizes) high-dimensional vectors into a lower-dimensional representation. These lower-dimensional vectors are non-negative which also means their coefficients are non-negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the nmf_model with the n_components = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 5\n",
    "\n",
    "nmf_model = NMF(random_state=40, n_components=num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_model.fit(dtm)\n",
    "len(tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the Top 20 words for each of the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words = np.array(tfidf.get_feature_names())\n",
    "topic_words_df = pd.DataFrame(np.zeros((num_topics, 20)), index=[f'Topic {i + 1}' for i in range(num_topics)],\n",
    "                           columns=[f'Word {i + 1}' for i in range(20)]).astype(str)\n",
    "\n",
    "for i in range(num_topics):\n",
    "    ix = nmf_model.components_[i].argsort()[::-1][:20]\n",
    "    topic_words_df.iloc[i] = words[ix]\n",
    "\n",
    "topic_words_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the best topic for each complaint in terms of integer value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_results = nmf_model.transform(dtm)\n",
    "topic_results.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new column Topic and assign the best topic to each of the complaint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['Topic'] = topic_results.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the first 5 Complaints for each of the Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "First5_comps=df_clean.groupby('Topic').head(5)\n",
    "First5_comps.sort_values('Topic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After evaluating the mapping, if the topics assigned are correct then assign these names to the relevant topic:\n",
    "- Bank Account services\n",
    "- Credit card or prepaid card\n",
    "- Theft/Dispute Reporting\n",
    "- Mortgage/Loan\n",
    "- Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dictionary of Topic names and Topics\n",
    "Topic_names = {0:'Bank account services', 1:'Others', 2:'Mortgage/Loan', 3:'Credit card or prepaid card', 4:'Theft/Dispute Reporting'}\n",
    "\n",
    "# Replace Topics with Topic Names\n",
    "df_clean['Topic'] = df_clean['Topic'].map(Topic_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Supervised Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dictionary again of Topic names and Topics\n",
    "\n",
    "Topic_names = {'Bank account services':0, 'Others':1, 'Mortgage/Loan':2, 'Credit card or prepaid card':3, 'Theft/Dispute Reporting':4}\n",
    "# Replace Topics with Topic Names\n",
    "df_clean['Topic'] = df_clean['Topic'].map(Topic_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = df_clean[['complaints_what_happened','Topic']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the supervised models on the training data created. In this process, we are going to do the following:\n",
    "- Create the vector counts using Count Vectoriser\n",
    "- Transform the word vecotr to tf-idf\n",
    "- Create the train & test data using the train_test_split on the tf-idf & topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(training_data.complaints_what_happened)\n",
    "\n",
    "# Write your code here to transform the word vector to tf-idf\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_tfidf, training_data.Topic, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression().fit(X_train, y_train)\n",
    "predicted_lr = lr.predict(X_test)\n",
    "\n",
    "print(\"Classification Report:-\")\n",
    "print(classification_report(y_test, predicted_lr))\n",
    "\n",
    "accuracy_lr=round(accuracy_score(y_test, predicted_lr),4)*100\n",
    "print(\"Accuracy of Logistic Regression:-\",accuracy_lr,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "predicted_dt = dt.predict(X_test)\n",
    "\n",
    "print(\"Classification Report:-\")\n",
    "print(classification_report(y_test, predicted_dt))\n",
    "\n",
    "accuracy_dt=round(accuracy_score(y_test, predicted_dt),4)*100\n",
    "print(\"Accuracy of Decision Trees-\",accuracy_dt,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "predicted_rf = rfc.predict(X_test)\n",
    "\n",
    "print(\"Classification Report:-\")\n",
    "print(classification_report(y_test, predicted_rf))\n",
    "\n",
    "accuracy_rf=round(accuracy_score(y_test, predicted_rf),4)*100\n",
    "print(\"Accuracy of Random Forest-\",accuracy_rf,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nb = GaussianNB().fit(X_train.toarray(), y_train)\n",
    "predicted_nb = nb.predict(X_test.toarray())\n",
    "\n",
    "print(\"Classification Report:-\")\n",
    "print(classification_report(y_test, predicted_nb))\n",
    "\n",
    "accuracy_nb=round(accuracy_score(y_test, predicted_nb),4)*100\n",
    "print(\"Accuracy of Gaussian Naive Bayes-\",accuracy_nb,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_summary_df=pd.DataFrame({'Model':['Logistic Regression','Decision Trees','Random Forest','Gaussian Naive Bayes'],\n",
    "                               'Accuracy (in %)':[accuracy_lr,accuracy_dt,accuracy_rf,accuracy_nb]})\n",
    "\n",
    "model_summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Thus by comapring the applied Machine Learning Algorithms, we can conclude that Logistic Regression Performs better than the other models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the Logistic Regression for Inference:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_topic(text):\n",
    "    Topic_names = {0:'Bank account services', 1:'Others', 2:'Mortgage/Loan', 3:'Credit card or prepaid card', 4:'Theft/Dispute Reporting'}\n",
    "    X_new_counts = count_vect.transform(text)\n",
    "    X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "    predicted = lr.predict(X_new_tfidf)\n",
    "    return Topic_names[predicted[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_complaints = pd.DataFrame({'complaints': [\"To whom it may concern, Chase bank charged wrongly overdraft fees, I have alert of low balance or unsuficent fee balance and always deposit immediatly to cover transactions if needed but Chase always changed the order and charged me overdraft fee anyway. when you call they said their per their guidelines they don't refund more then 2 overdrawft doesn't matter bank fault or not.Taken {$34.00} from people is money just because you can is not Ok.See attached documents. When Chase refund, they always find the way to take back what they refunded in first place.\",\n",
    "                                            \"Chase is marketing credit cards to those of us with good credit like it's going out of style. Be careful - the marketing is not clear. IF you already have a SWA Chase personal card, do not apply for the new one online. This has sent me through a XXXX triangle, wasting my time and therefore money.It appears in the middle of XX/XX/2018, Chase expanded their undisclosed 5/24 rule to include ALL co-branded cards, not just SWA, yet the marketing machine continues to ignore this policy and the Agents on the phone are not well trained. I am contacting the credit bureau b/c two of the cards opened are not mine - that is the silver lining here. However, the issue with incessant marketing of their branded cards to customers, plus this confusing 5/24 rule and lack of Agent phone training, is false advertising. The information is still relatively opaque, as Chase never comments on the 5/24 rule, but multiple reader and community data points suggest denials due to 5/24 for cards previously exempt. CFPB staff, please help educate consumers and hold the big banks accountable for deceptive trade practices. I do not believe this is intentional on Chase 's part, but the second to last Agent did encourage me to file a complaint here, so be it.\",\n",
    "                                            \"What is the procedure to know my CIBIL score?\",\n",
    "                                            \"I can not get from chase who services my mortgage, who owns it and who has original loan docs\", \n",
    "                                  \"The bill amount of my credit card was debited twice. Please look into the matter and resolve at the earliest.\",\n",
    "                                  \"I want to open a salary account at your downtown branch. Please provide me the procedure.\",\n",
    "                                  \"Yesterday, I received a fraudulent email regarding renewal of my services.\",\n",
    "                                            \"Where are the bank branches in the city of Mumbai?\",\n",
    "                                            \"unwanted service activated and money deducted automatically\"]})\n",
    "df_complaints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions on new complaints:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_complaints['Predicted Topic'] = df_complaints['complaints'].apply(lambda x: predict_topic([x]))\n",
    "df_complaints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
